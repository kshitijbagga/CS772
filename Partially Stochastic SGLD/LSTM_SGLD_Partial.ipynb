{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11639033,"sourceType":"datasetVersion","datasetId":7303071},{"sourceId":11643336,"sourceType":"datasetVersion","datasetId":7306186}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6f233884-5d03-4f0d-8cf9-1b0fbc662201","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import ClippedAdam\nimport numpy as np\nimport pandas as pd\nimport glob, os\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:56.185673Z","iopub.execute_input":"2025-05-02T03:19:56.186240Z","iopub.status.idle":"2025-05-02T03:19:58.633816Z","shell.execute_reply.started":"2025-05-02T03:19:56.186216Z","shell.execute_reply":"2025-05-02T03:19:58.633111Z"}},"outputs":[],"execution_count":1},{"id":"9b47bb43-3d41-4d24-a9b3-8655954861ea","cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:58.635027Z","iopub.execute_input":"2025-05-02T03:19:58.635462Z","iopub.status.idle":"2025-05-02T03:19:58.692136Z","shell.execute_reply.started":"2025-05-02T03:19:58.635415Z","shell.execute_reply":"2025-05-02T03:19:58.691322Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"id":"2ae68501-43b2-4ed3-bf4a-1d25a7076b64","cell_type":"code","source":"# --- Load and prepare data ---\ndef load_data(data_dir, seq_len=18):\n    paths = sorted(\n        glob.glob(os.path.join(data_dir, \"IPF_Final_*.csv\")),\n        key=lambda p: int(os.path.basename(p).split('_')[2].split('.')[0])\n    )\n    assert len(paths) >= seq_len + 1, f\"Need at least {seq_len + 1} CSV files for full sequence + target.\"\n\n    arrays = [np.loadtxt(path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) for path in paths[:seq_len + 1]]\n    X_seq = np.stack(arrays[:seq_len], axis=0)\n    Y_arr = arrays[seq_len]\n\n    x_tensor = torch.tensor(X_seq.transpose(1, 0, 2), dtype=torch.float32)\n    y_tensor = torch.tensor(Y_arr, dtype=torch.float32)\n    return x_tensor, y_tensor\n\ndata_dir = \"/kaggle/input/traincs772\"\nseq_len = 18\nx_full, y_full = load_data(data_dir, seq_len)\nx_full = x_full.to(device)\ny_full = y_full.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:58.692940Z","iopub.execute_input":"2025-05-02T03:19:58.693148Z","iopub.status.idle":"2025-05-02T03:19:58.962048Z","shell.execute_reply.started":"2025-05-02T03:19:58.693131Z","shell.execute_reply":"2025-05-02T03:19:58.961468Z"}},"outputs":[],"execution_count":3},{"id":"82ffe3f8-e066-4bbe-870b-ff4f4080db0f","cell_type":"code","source":"class MultiLSTMBackbone(nn.Module):\n    def __init__(self, input_size=3, hidden_size=64):\n        super().__init__()\n        self.lstm_phi1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.lstm_phi = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.lstm_phi2 = nn.LSTM(input_size, hidden_size, batch_first=True)\n\n        self.fc_phi1 = nn.Linear(hidden_size * 3, 1)\n        self.fc_phi = nn.Linear(hidden_size * 3, 1)\n        self.fc_phi2 = nn.Linear(hidden_size * 3, 1)\n\n    def forward(self, x):\n        _, (h1, _) = self.lstm_phi1(x)\n        _, (h2, _) = self.lstm_phi(x)\n        _, (h3, _) = self.lstm_phi2(x)\n        h_cat = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1)\n        out1 = self.fc_phi1(h_cat)\n        out2 = self.fc_phi(h_cat)\n        out3 = self.fc_phi2(h_cat)\n        return out1, out2, out3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:58.963610Z","iopub.execute_input":"2025-05-02T03:19:58.963834Z","iopub.status.idle":"2025-05-02T03:19:58.969614Z","shell.execute_reply.started":"2025-05-02T03:19:58.963815Z","shell.execute_reply":"2025-05-02T03:19:58.968887Z"}},"outputs":[],"execution_count":4},{"id":"04442118-ed8e-40ee-b859-b6a0bafbb0e8","cell_type":"code","source":"def model_fn(x, y=None):\n    y1, y2, y3 = (y[:, 0:1], y[:, 1:2], y[:, 2:3]) if y is not None else (None, None, None)\n\n    _, (h1, _) = model.lstm_phi1(x)\n    _, (h2, _) = model.lstm_phi(x)\n    _, (h3, _) = model.lstm_phi2(x)\n    h_combined = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1)\n\n    w1 = pyro.sample(\"w1\", dist.Normal(model.fc_phi1.weight, 0.5).to_event(2))\n    b1 = pyro.sample(\"b1\", dist.Normal(model.fc_phi1.bias, 0.5).to_event(1))\n    w2 = pyro.sample(\"w2\", dist.Normal(model.fc_phi.weight, 0.5).to_event(2))\n    b2 = pyro.sample(\"b2\", dist.Normal(model.fc_phi.bias, 0.5).to_event(1))\n    w3 = pyro.sample(\"w3\", dist.Normal(model.fc_phi2.weight, 0.5).to_event(2))\n    b3 = pyro.sample(\"b3\", dist.Normal(model.fc_phi2.bias, 0.5).to_event(1))\n\n    mean1 = h_combined @ w1.t() + b1\n    mean2 = h_combined @ w2.t() + b2\n    mean3 = h_combined @ w3.t() + b3\n\n    with pyro.plate(\"data\", x.shape[0]):\n        pyro.sample(\"obs1\", dist.Normal(mean1, 5.0).to_event(1), obs=y1)\n        pyro.sample(\"obs2\", dist.Normal(mean2, 5.0).to_event(1), obs=y2)\n        pyro.sample(\"obs3\", dist.Normal(mean3, 5.0).to_event(1), obs=y3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:58.970248Z","iopub.execute_input":"2025-05-02T03:19:58.970488Z","iopub.status.idle":"2025-05-02T03:19:58.984985Z","shell.execute_reply.started":"2025-05-02T03:19:58.970470Z","shell.execute_reply":"2025-05-02T03:19:58.984343Z"}},"outputs":[],"execution_count":5},{"id":"4d7423b3-3082-4e53-9c0e-0dd3f93a65fa","cell_type":"code","source":"def guide_fn(x, y=None):\n    for i, head in enumerate([model.fc_phi1, model.fc_phi, model.fc_phi2], start=1):\n        pyro.param(f\"w{i}_loc\", head.weight.detach().clone())\n        pyro.param(f\"w{i}_scale\", torch.ones_like(head.weight) * 0.5, constraint=dist.constraints.positive)\n        pyro.param(f\"b{i}_loc\", head.bias.detach().clone())\n        pyro.param(f\"b{i}_scale\", torch.ones_like(head.bias) * 0.5, constraint=dist.constraints.positive)\n\n        pyro.sample(f\"w{i}\", dist.Normal(pyro.param(f\"w{i}_loc\"), pyro.param(f\"w{i}_scale\")).to_event(2))\n        pyro.sample(f\"b{i}\", dist.Normal(pyro.param(f\"b{i}_loc\"), pyro.param(f\"b{i}_scale\")).to_event(1))\n\n\n# Instantiate model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MultiLSTMBackbone().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:58.985637Z","iopub.execute_input":"2025-05-02T03:19:58.985888Z","iopub.status.idle":"2025-05-02T03:19:59.020231Z","shell.execute_reply.started":"2025-05-02T03:19:58.985870Z","shell.execute_reply":"2025-05-02T03:19:59.019786Z"}},"outputs":[],"execution_count":6},{"id":"7348b6a8-6f0f-4c38-86b6-5d426c8a3f53","cell_type":"code","source":"# --- LSTM Backbone Pretraining ---\nprint(\"ðŸ”§ Pretraining deterministic LSTM backbone...\")\nbackbone_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.MSELoss()\n\nmodel.train()\nfor epoch in range(2000):\n    backbone_optimizer.zero_grad()\n    out1, out2, out3 = model(x_full)\n    loss = (\n        0.33 * loss_fn(out1, y_full[:, 0:1]) +\n        0.33 * loss_fn(out2, y_full[:, 1:2]) +\n        0.34 * loss_fn(out3, y_full[:, 2:3])\n    )\n    loss.backward()\n    backbone_optimizer.step()\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch + 1} LSTM Backbone Loss: {loss:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:59.020928Z","iopub.execute_input":"2025-05-02T03:19:59.021175Z","iopub.status.idle":"2025-05-02T03:22:01.603820Z","shell.execute_reply.started":"2025-05-02T03:19:59.021153Z","shell.execute_reply":"2025-05-02T03:22:01.603069Z"}},"outputs":[{"name":"stdout","text":"ðŸ”§ Pretraining deterministic LSTM backbone...\nEpoch 100 LSTM Backbone Loss: 6546.93\nEpoch 200 LSTM Backbone Loss: 4735.44\nEpoch 300 LSTM Backbone Loss: 3853.09\nEpoch 400 LSTM Backbone Loss: 3347.92\nEpoch 500 LSTM Backbone Loss: 3042.70\nEpoch 600 LSTM Backbone Loss: 2830.90\nEpoch 700 LSTM Backbone Loss: 2670.02\nEpoch 800 LSTM Backbone Loss: 2536.49\nEpoch 900 LSTM Backbone Loss: 2409.76\nEpoch 1000 LSTM Backbone Loss: 2280.72\nEpoch 1100 LSTM Backbone Loss: 2163.22\nEpoch 1200 LSTM Backbone Loss: 2050.39\nEpoch 1300 LSTM Backbone Loss: 1939.17\nEpoch 1400 LSTM Backbone Loss: 1832.63\nEpoch 1500 LSTM Backbone Loss: 1727.07\nEpoch 1600 LSTM Backbone Loss: 1596.29\nEpoch 1700 LSTM Backbone Loss: 1488.92\nEpoch 1800 LSTM Backbone Loss: 1380.52\nEpoch 1900 LSTM Backbone Loss: 1277.77\nEpoch 2000 LSTM Backbone Loss: 1185.04\n","output_type":"stream"}],"execution_count":7},{"id":"eff7eeae-6048-4119-a6d0-5691f21c1987","cell_type":"code","source":"# import math\n\n# # --- Manual SGLD for the three linear heads ---\n\n# # 1) Freeze your backbone\n# for p in model.parameters():\n#     p.requires_grad = False\nmodel.eval()\n\n# 2) Precompute the combined hidden representation for all data\nwith torch.no_grad():\n    _, (h1, _) = model.lstm_phi1(x_full)\n    _, (h2, _) = model.lstm_phi(x_full)\n    _, (h3, _) = model.lstm_phi2(x_full)\n    h_combined = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1)  # shape (N, hidden*3)\n    y_true = y_full  # shape (N, 3)\n\n# 3) Initialize SGLD parameters from your pretrained heads\nw1 = model.fc_phi1.weight.detach().clone().requires_grad_(True)\nb1 = model.fc_phi1.bias.detach().clone().requires_grad_(True)\nw2 = model.fc_phi.weight.detach().clone().requires_grad_(True)\nb2 = model.fc_phi.bias.detach().clone().requires_grad_(True)\nw3 = model.fc_phi2.weight.detach().clone().requires_grad_(True)\nb3 = model.fc_phi2.bias.detach().clone().requires_grad_(True)\n\n# 4) SGLD hyperparameters\nlr = 1e-5            # stepâ€size (try between 1e-6 and 1e-4)\nsigma_lik = 5.0      # your observation noise Ïƒ in the Normal likelihood\ntau0 = 0.5           # prior std dev for weights/biases\nn_iters = 5000\nburn_in = 1000\nthin = 10\n\n# 5) Storage for posterior samples\nsamples = {'w1':[], 'b1':[], 'w2':[], 'b2':[], 'w3':[], 'b3':[]}\n\n# 6) Run SGLD\nfor it in range(n_iters):\n    # a) Compute negative log-posterior (up to constant)\n    phi1 = h_combined @ w1.t() + b1     # (N,1)\n    phi2 = h_combined @ w2.t() + b2\n    phi3 = h_combined @ w3.t() + b3\n\n    mse_term = ((y_true[:,0:1] - phi1)**2).sum() \\\n             + ((y_true[:,1:2] - phi2)**2).sum() \\\n             + ((y_true[:,2:3] - phi3)**2).sum()\n    neg_log_like = mse_term / (2 * sigma_lik**2)\n\n    prior_wb = (w1**2).sum() + (b1**2).sum() \\\n             + (w2**2).sum() + (b2**2).sum() \\\n             + (w3**2).sum() + (b3**2).sum()\n    neg_log_prior = prior_wb / (2 * tau0**2)\n\n    loss = neg_log_like + neg_log_prior\n    loss.backward()\n\n    # b) SGLD update: Î¸ â† Î¸ âˆ’ Î· âˆ‡Î¸ + âˆš(2Î·) Î¾\n    with torch.no_grad():\n        for name, param in [('w1', w1), ('b1', b1),\n                            ('w2', w2), ('b2', b2),\n                            ('w3', w3), ('b3', b3)]:\n            grad = param.grad\n            noise = torch.randn_like(param) * math.sqrt(2 * lr)\n            param -= lr * grad\n            param += noise\n            param.grad.zero_()\n\n    # c) Collect after burn-in & thinning\n    if it >= burn_in and (it - burn_in) % thin == 0:\n        samples['w1'].append(w1.clone())\n        samples['b1'].append(b1.clone())\n        samples['w2'].append(w2.clone())\n        samples['b2'].append(b2.clone())\n        samples['w3'].append(w3.clone())\n        samples['b3'].append(b3.clone())\n\n# 7) Compute posterior means\nw1_post = torch.stack(samples['w1'], dim=0).mean(0)\nb1_post = torch.stack(samples['b1'], dim=0).mean(0)\nw2_post = torch.stack(samples['w2'], dim=0).mean(0)\nb2_post = torch.stack(samples['b2'], dim=0).mean(0)\nw3_post = torch.stack(samples['w3'], dim=0).mean(0)\nb3_post = torch.stack(samples['b3'], dim=0).mean(0)\n\n# --- Inference with these posterior means ---\nwith torch.no_grad():\n    phi1 = h_combined @ w1_post.t() + b1_post\n    phi2 = h_combined @ w2_post.t() + b2_post\n    phi3 = h_combined @ w3_post.t() + b3_post\n    preds = torch.cat([phi1, phi2, phi3], dim=1).cpu().numpy()\n\nrmse = np.sqrt(np.mean((preds - y_true.cpu().numpy())**2))\nprint(f\"âœ… Final Manual-SGLD RMSE: {rmse:.4f} degrees\")\n\n# 8) Save as CSV\ndf = pd.DataFrame(preds, columns=[\"Phi1\",\"Phi\",\"Phi2\"])\ndf.insert(0, \"Phase\", 1)\ndf.to_csv(\"IPF_20_pred_manual_sgld.csv\", index=False)\nprint(\"âœ… Manual SGLD predictions saved to IPF_20_pred_manual_sgld.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:51:36.192076Z","iopub.execute_input":"2025-05-02T03:51:36.192412Z","iopub.status.idle":"2025-05-02T03:51:46.741486Z","shell.execute_reply.started":"2025-05-02T03:51:36.192386Z","shell.execute_reply":"2025-05-02T03:51:46.740700Z"}},"outputs":[{"name":"stdout","text":"âœ… Final Manual-SGLD RMSE: 14.7996 degrees\nâœ… Manual SGLD predictions saved to IPF_20_pred_manual_sgld.csv\n","output_type":"stream"}],"execution_count":17},{"id":"40ce22a1-4d98-4761-b152-f479467cf0db","cell_type":"code","source":"import math\nimport os\nimport pandas as pd\nimport torch\n\n# Ensure model, x_full, y_full are already defined and on the correct device\n# model: your MultiLSTMBackbone instance\n# x_full: input tensor of shape (N, seq_len, features)\n# y_full: target tensor of shape (N, 3)\n\n# --- Fully Stochastic SGLD over the entire network ---\n# Hyperparameters\nlr        = 1e-5    # SGLD step size\nsigma_lik = 5.0     # observation noise std dev\ntau0      = 0.5     # prior std dev on all weights\nn_iters   = 5000    # total SGLD iterations\nburn_in   = 1000    # burn-in period\nthin      = 10      # thinning interval\n\n# 1) Collect all trainable parameters\nall_params  = list(model.parameters())\nparam_names = [f\"p{i}\" for i in range(len(all_params))]\n# buffer to store samples\nsamples     = {name: [] for name in param_names}\n\n# 2) Run SGLD\nfor it in range(n_iters):\n    # a) Full forward pass\n    out1, out2, out3 = model(x_full)                   # each shape (N,1)\n    y_pred = torch.cat([out1, out2, out3], dim=1)      # shape (N,3)\n\n    # b) Negative log-likelihood term\n    mse = ((y_full - y_pred)**2).sum()\n    neg_log_like = mse / (2 * sigma_lik**2)\n\n    # c) Negative log-prior term (Gaussian prior on all params)\n    prior_norm2 = sum((p**2).sum() for p in all_params)\n    neg_log_prior = prior_norm2 / (2 * tau0**2)\n\n    # d) Total loss = negative log-posterior\n    loss = neg_log_like + neg_log_prior\n    loss.backward()\n\n    # e) SGLD update step: drift + diffusion\n    with torch.no_grad():\n        for name, param in zip(param_names, all_params):\n            # gradient drift\n            param -= lr * param.grad\n            # injected Gaussian noise\n            noise = torch.randn_like(param) * math.sqrt(2 * lr)\n            param += noise\n            # reset gradient\n            param.grad.zero_()\n\n    # f) Collect samples after burn-in & according to thinning\n    if it >= burn_in and (it - burn_in) % thin == 0:\n        for name, param in zip(param_names, all_params):\n            samples[name].append(param.clone())\n\n# 3) Compute posterior-mean for each parameter\npost_means = {\n    name: torch.stack(vals, dim=0).mean(0)\n    for name, vals in samples.items()\n}\n\n# 4) Overwrite model parameters with posterior means\nwith torch.no_grad():\n    for param, name in zip(all_params, param_names):\n        param.copy_(post_means[name])\n\n# 5) Final inference & saving predictions\nmodel.eval()\nwith torch.no_grad():\n    out1, out2, out3 = model(x_full)\n    preds = torch.cat([out1, out2, out3], dim=1).cpu().numpy()\n\n# Prepare DataFrame and save\nos.makedirs('/mnt/data', exist_ok=True)\ndf = pd.DataFrame(preds, columns=[\"Phi1\",\"Phi\",\"Phi2\"])\ndf.insert(0, \"Phase\", 1)\noutput_path = '/mnt/data/IPF_20_pred_full_sgld.csv'\ndf.to_csv(output_path, index=False)\nprint(f\"âœ… Saved fully stochastic SGLD predictions to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:58:38.312927Z","iopub.execute_input":"2025-05-02T03:58:38.313462Z","iopub.status.idle":"2025-05-02T03:58:38.339862Z","shell.execute_reply.started":"2025-05-02T03:58:38.313417Z","shell.execute_reply":"2025-05-02T03:58:38.338837Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_108/3535282945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# d) Total loss = negative log-posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_log_like\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_log_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# e) SGLD update step: drift + diffusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}],"execution_count":20}]}