{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11639033,"sourceType":"datasetVersion","datasetId":7303071}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pyro-ppl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:25:59.155337Z","iopub.execute_input":"2025-05-02T14:25:59.155606Z","iopub.status.idle":"2025-05-02T14:26:02.214798Z","shell.execute_reply.started":"2025-05-02T14:25:59.155585Z","shell.execute_reply":"2025-05-02T14:26:02.214055Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.11/dist-packages (1.9.1)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (3.4.0)\nRequirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (0.1.2)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (2.5.1+cu124)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->pyro-ppl) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F # Import F for linear\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroSample, PyroModule\nfrom torch.distributions import constraints\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom pyro.optim import ClippedAdam\nimport numpy as np\nimport pandas as pd\nimport glob\nimport os\nfrom sklearn.metrics import mean_squared_error\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:26:02.216609Z","iopub.execute_input":"2025-05-02T14:26:02.216859Z","iopub.status.idle":"2025-05-02T14:26:04.707302Z","shell.execute_reply.started":"2025-05-02T14:26:02.216834Z","shell.execute_reply":"2025-05-02T14:26:04.706510Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- Configuration ---\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data and Model Parameters\nDATA_DIR = \"/kaggle/input/training/Training Dataset\" # Directory containing the CSV files\nPREDICTION_FILE_INDEX = 20 # Index of the file to predict (e.g., 20 for IPF_Final_20.csv)\nSEQ_LEN = 18 # Length of the input sequence\nHIDDEN_DIM = 64\nOUTPUT_DIM = 3\nINPUT_DIM = 3 # Corresponds to the 3 columns used (cols 1, 2, 3)\n\n# Training Parameters\nNUM_STEPS = 2000\nLEARNING_RATE = 1e-3\nPATIENCE = 100 # For early stopping\nMIN_DELTA = 1.0 # Minimum improvement for early stopping\nNUM_PREDICTION_SAMPLES = 100 # Number of samples from posterior for prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:26:04.708161Z","iopub.execute_input":"2025-05-02T14:26:04.708484Z","iopub.status.idle":"2025-05-02T14:26:04.768112Z","shell.execute_reply.started":"2025-05-02T14:26:04.708466Z","shell.execute_reply":"2025-05-02T14:26:04.767037Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Data Loading ---\n\ndef get_file_paths(data_dir):\n    \"\"\"Gets and sorts file paths based on the index in the filename.\"\"\"\n    paths = glob.glob(os.path.join(data_dir, \"IPF_Final_*.csv\"))\n    # Sort files numerically based on the index after 'IPF_Final_'\n    paths.sort(key=lambda p: int(os.path.basename(p).split('_')[2].split('.')[0]))\n    return paths\n\ndef load_training_data(paths, seq_len):\n    \"\"\"Loads sequence data for training.\"\"\"\n    print(f\"Loading training data using sequence length: {seq_len}\")\n    # Need seq_len files for input and 1 file for the target\n    num_files_needed = seq_len + 1\n    if len(paths) < num_files_needed:\n        raise ValueError(f\"Need at least {num_files_needed} CSV files for training, but found {len(paths)}.\")\n\n    # Load the first seq_len+1 files\n    # Ensure files exist before attempting to load\n    for path in paths[:num_files_needed]:\n        if not os.path.exists(path):\n             raise FileNotFoundError(f\"Required training file not found: {path}\")\n\n    arrays = [np.loadtxt(path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) for path in paths[:num_files_needed]]\n\n    # Stack the first seq_len arrays for the input sequence\n    X_seq = np.stack(arrays[:seq_len], axis=0) # Shape: (seq_len, num_samples, input_dim)\n    # The (seq_len+1)-th array is the target\n    Y_arr = arrays[seq_len] # Shape: (num_samples, output_dim)\n\n    # Transpose X_seq to (num_samples, seq_len, input_dim) as expected by LSTM batch_first=True\n    x_tensor = torch.tensor(X_seq.transpose(1, 0, 2), dtype=torch.float32).to(device)\n    y_tensor = torch.tensor(Y_arr, dtype=torch.float32).to(device)\n    print(f\"Training X shape: {x_tensor.shape}, Training Y shape: {y_tensor.shape}\")\n    return x_tensor, y_tensor\n\ndef load_prediction_data(paths, seq_len, prediction_file_index):\n    \"\"\"Loads the input sequence needed for prediction and the target file.\"\"\"\n    print(f\"Loading data for predicting file index: {prediction_file_index}\")\n    # Indices are 1-based in filenames, list indices are 0-based\n    target_file_name = f\"IPF_Final_{prediction_file_index}.csv\"\n    target_file_path = os.path.join(DATA_DIR, target_file_name)\n\n    if target_file_path not in paths:\n          raise FileNotFoundError(f\"Target file {target_file_path} not found in the dataset.\")\n\n    # Find the index of the target file in the sorted list\n    target_list_index = paths.index(target_file_path)\n\n    # The input sequence consists of seq_len files *before* the target file\n    start_index = target_list_index - seq_len\n    end_index = target_list_index # Exclusive\n\n    if start_index < 0:\n        # Find the first available index to make the sequence start from there\n        actual_seq_len = target_list_index\n        if actual_seq_len == 0:\n             raise ValueError(f\"Prediction file {prediction_file_index} is the first file. Cannot form a sequence of length {seq_len}.\")\n        print(f\"Warning: Not enough preceding files to form a sequence of length {seq_len} for prediction file {prediction_file_index}.\")\n        print(f\"Using a shorter sequence of length {actual_seq_len} starting from file index {paths[0].split('_')[-1].split('.')[0]}.\")\n        start_index = 0 # Start from the first available file\n        # Note: This will use a shorter sequence than SEQ_LEN if start_index < 0 originally.\n        # The LSTM handles variable length sequences, but training is done with fixed SEQ_LEN.\n        # This might lead to a train/predict mismatch if sequence length is critical.\n        # For robustness, you might want to require sufficient preceding files for prediction too.\n        # Let's keep the original error behavior to maintain consistency with expected input sequence length.\n        raise ValueError(f\"Not enough preceding files ({target_list_index}) to form a sequence of length {seq_len} for prediction file {prediction_file_index}.\")\n\n\n    input_paths = paths[start_index:end_index]\n    print(f\"Using files {os.path.basename(input_paths[0])} to {os.path.basename(input_paths[-1])} for prediction input.\")\n\n    # Ensure input files exist\n    for path in input_paths:\n         if not os.path.exists(path):\n             raise FileNotFoundError(f\"Required prediction input file not found: {path}\")\n\n    # Load the sequence data\n    input_arrays = [np.loadtxt(path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) for path in input_paths]\n    X_pred_seq = np.stack(input_arrays, axis=0) # Shape: (seq_len, num_samples, input_dim)\n\n    # Load the target data\n    Y_pred_arr = np.loadtxt(target_file_path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) # Shape: (num_samples, output_dim)\n\n    # Transpose X_pred_seq for LSTM input\n    x_pred_tensor = torch.tensor(X_pred_seq.transpose(1, 0, 2), dtype=torch.float32).to(device)\n    y_pred_tensor = torch.tensor(Y_pred_arr, dtype=torch.float32).to(device)\n    print(f\"Prediction X shape: {x_pred_tensor.shape}, Prediction Y shape: {y_pred_tensor.shape}\")\n    return x_pred_tensor, y_pred_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:26:04.769009Z","iopub.execute_input":"2025-05-02T14:26:04.769320Z","iopub.status.idle":"2025-05-02T14:26:04.783046Z","shell.execute_reply.started":"2025-05-02T14:26:04.769263Z","shell.execute_reply":"2025-05-02T14:26:04.782463Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_prediction_data(paths, seq_len, prediction_file_index):\n    \"\"\"Loads the input sequence needed for prediction and the target file.\"\"\"\n    print(f\"Loading data for predicting file index: {prediction_file_index}\")\n    # Indices are 1-based in filenames, list indices are 0-based\n    target_file_path = os.path.join(DATA_DIR, f\"IPF_Final_{prediction_file_index}.csv\")\n\n    if target_file_path not in paths:\n         raise FileNotFoundError(f\"Target file {target_file_path} not found in the dataset.\")\n\n    # Find the index of the target file in the sorted list\n    target_list_index = paths.index(target_file_path)\n\n    # The input sequence consists of seq_len files *before* the target file\n    start_index = target_list_index - seq_len\n    end_index = target_list_index # Exclusive\n\n    if start_index < 0:\n        raise ValueError(f\"Not enough preceding files ({start_index}) to form a sequence of length {seq_len} for prediction file {prediction_file_index}.\")\n\n    input_paths = paths[start_index:end_index]\n    print(f\"Using files {os.path.basename(input_paths[0])} to {os.path.basename(input_paths[-1])} for prediction input.\")\n\n    # Load the sequence data\n    input_arrays = [np.loadtxt(path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) for path in input_paths]\n    X_pred_seq = np.stack(input_arrays, axis=0) # Shape: (seq_len, num_samples, input_dim)\n\n    # Load the target data\n    Y_pred_arr = np.loadtxt(target_file_path, delimiter=',', skiprows=1, usecols=(1, 2, 3)) # Shape: (num_samples, output_dim)\n\n    # Transpose X_pred_seq for LSTM input\n    x_pred_tensor = torch.tensor(X_pred_seq.transpose(1, 0, 2), dtype=torch.float32).to(device)\n    y_pred_tensor = torch.tensor(Y_pred_arr, dtype=torch.float32).to(device)\n    print(f\"Prediction X shape: {x_pred_tensor.shape}, Prediction Y shape: {y_pred_tensor.shape}\")\n    return x_pred_tensor, y_pred_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:26:04.784642Z","iopub.execute_input":"2025-05-02T14:26:04.785196Z","iopub.status.idle":"2025-05-02T14:26:04.796718Z","shell.execute_reply.started":"2025-05-02T14:26:04.785177Z","shell.execute_reply":"2025-05-02T14:26:04.796109Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- Model Definition ---\n\nclass MultiLSTMBackbone(PyroModule):\n    \"\"\"\n    A Bayesian LSTM backbone using multiple LSTMs and combining their outputs.\n    Weights and biases of LSTMs and FC layers are treated as random variables.\n    Uses PyroSample for weights/biases and F.linear for the linear transformation.\n    \"\"\"\n    def __init__(self, input_size=3, hidden_size=64):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        # Calculate input/output sizes for the linear layers\n        hidden_fc_input_size = hidden_size * 3\n        fc_output_size = 1 # Each linear layer outputs 1 dimension\n\n        # Define Bayesian LSTM layers using PyroModule\n        self.lstm_phi1 = PyroModule[nn.LSTM](input_size, hidden_size, batch_first=True)\n        # The default PyroSample priors for nn.LSTM should be fine here\n        # (handled automatically by PyroModule[nn.LSTM])\n\n        self.lstm_phi = PyroModule[nn.LSTM](input_size, hidden_size, batch_first=True)\n         # The default PyroSample priors for nn.LSTM should be fine here\n\n        self.lstm_phi2 = PyroModule[nn.LSTM](input_size, hidden_size, batch_first=True)\n         # The default PyroSample priors for nn.LSTM should be fine here\n\n        # Define Bayesian FC layer parameters explicitly using PyroSample\n        # We will use F.linear in the forward pass with these sampled parameters.\n        # This avoids potential issues with PyroModule[nn.Linear]'s internal handling.\n        self.fc1_weight = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, hidden_fc_input_size, device=device),\n            torch.ones(fc_output_size, hidden_fc_input_size, device=device)\n        ).to_event(2)) # Sample shape (1, 192), treat as single variable\n\n        self.fc1_bias = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, device=device),\n            torch.ones(fc_output_size, device=device)\n        ).to_event(1)) # Sample shape (1,), treat as single variable\n\n        self.fc2_weight = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, hidden_fc_input_size, device=device),\n            torch.ones(fc_output_size, hidden_fc_input_size, device=device)\n        ).to_event(2)) # Sample shape (1, 192), treat as single variable\n\n        self.fc2_bias = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, device=device),\n            torch.ones(fc_output_size, device=device)\n        ).to_event(1)) # Sample shape (1,), treat as single variable\n\n        self.fc3_weight = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, hidden_fc_input_size, device=device),\n            torch.ones(fc_output_size, hidden_fc_input_size, device=device)\n        ).to_event(2)) # Sample shape (1, 192), treat as single variable\n\n        self.fc3_bias = PyroSample(lambda self: dist.Normal(\n            torch.zeros(fc_output_size, device=device),\n            torch.ones(fc_output_size, device=device)\n        ).to_event(1)) # Sample shape (1,), treat as single variable\n\n\n    def forward(self, x):\n        \"\"\"Forward pass through the multi-LSTM backbone.\"\"\"\n        # Ensure input is on the same device as model parameters\n        # Use a parameter's device that is guaranteed to exist (e.g., one of the sampled weights)\n        try:\n            target_device = self.fc1_weight.device\n        except AttributeError:\n             if pyro.settings.get(\"enable_validation\", False):\n                  print(\"Warning: Could not access self.fc1_weight.device. This might happen during early tracing or if samples are not yet available. Falling back to global device.\")\n             target_device = device\n\n        x = x.to(target_device)\n\n\n        # Pass input through each LSTM\n        # Note: If you wanted Bayesian LSTMs *without* PyroModule[nn.LSTM],\n        # you would sample weight/bias matrices here and use F.lstm, similar to F.linear below.\n        # PyroModule[nn.LSTM] handles sampling its parameters internally in the model.\n        _, (h1, _) = self.lstm_phi1(x) # h1 shape: (1, batch_size, hidden_size)\n        _, (h2, _) = self.lstm_phi(x)\n        _, (h3, _) = self.lstm_phi2(x)\n\n        # Concatenate the last hidden states from each LSTM\n        # h_n[-1] accesses the hidden state of the last layer\n        h_cat = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1) # Shape: (batch_size, hidden_size * 3)\n\n        # Pass concatenated hidden states through the linear layers using sampled parameters\n        # F.linear handles the batch dimension correctly.\n        out1 = F.linear(h_cat, self.fc1_weight, self.fc1_bias) # Shape: (batch_size, 1)\n        out2 = F.linear(h_cat, self.fc2_weight, self.fc2_bias) # Shape: (batch_size, 1)\n        out3 = F.linear(h_cat, self.fc3_weight, self.fc3_bias) # Shape: (batch_size, 1)\n\n        # Concatenate the outputs of the FC layers\n        final_out = torch.cat([out1, out2, out3], dim=1) # Shape: (batch_size, 3)\n        return final_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:43:52.315856Z","iopub.execute_input":"2025-05-02T14:43:52.316123Z","iopub.status.idle":"2025-05-02T14:43:52.332421Z","shell.execute_reply.started":"2025-05-02T14:43:52.316107Z","shell.execute_reply":"2025-05-02T14:43:52.331704Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class BayesianLSTMModel(PyroModule):\n    \"\"\"\n    The complete Bayesian LSTM model including the backbone and observation likelihood.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        # Instantiate the backbone network\n        self.backbone = MultiLSTMBackbone(input_dim, hidden_dim)\n\n    def forward(self, x, y=None):\n        \"\"\"\n        Generative model p(y, z | x) where z are the latent parameters.\n        Defines the generative process:\n        1. Samples the parameters z (implicitly via PyroSample in backbone).\n        2. Runs the input `x` through the backbone to get mean predictions.\n        3. Samples observation noise scale `obs_scale`.\n        4. Samples the output `y` from a Normal distribution centered at the backbone's output.\n        \"\"\"\n        # Get the mean prediction from the backbone\n        # This call implicitly samples parameters defined with PyroSample in the backbone\n        out_mean = self.backbone(x) # Shape: (batch_size, output_dim)\n\n        # Ensure output mean is on the correct device before sampling obs_scale\n        out_mean = out_mean.to(device)\n\n        # Sample observation noise standard deviation (sigma)\n        # Sample one observation noise standard deviation *per output dimension*, shared across batch.\n        # .to_event(1) ensures the sample site 'obs_scale' is treated as a single random variable (a vector of scales)\n        # Sampled OUTSIDE the data plate.\n        obs_scale = pyro.sample(\"obs_scale\",\n                                dist.Uniform(0.5, 2.0).expand([self.output_dim]).to_event(1)\n                               ).to(out_mean.device) # Ensure scale is on same device\n\n        # Plate over the batch dimension for the observation likelihood\n        with pyro.plate(\"data\", size=x.size(0)):\n            # Expand scale to match the output shape for the Normal distribution\n            # Since obs_scale is shape (output_dim,), expanding it to out_mean.shape (batch_size, output_dim)\n            # will tile it across the batch dimension, which is correct for shared scales.\n            scale_expanded = obs_scale.expand(out_mean.shape)\n\n            # Sample the observed data `y` using a Normal likelihood\n            # The `to_event(1)` treats the output dimensions for a single data point as conditionally independent\n            # given the mean (from the backbone) and the scale vector.\n            pyro.sample(\"obs\",\n                        dist.Normal(out_mean, scale_expanded).to_event(1),\n                        obs=y) # Pass observed data `y` if training/evaluating likelihood\n\n        return out_mean # Return the mean prediction (useful for prediction)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:43:58.089171Z","iopub.execute_input":"2025-05-02T14:43:58.089465Z","iopub.status.idle":"2025-05-02T14:43:58.095811Z","shell.execute_reply.started":"2025-05-02T14:43:58.089442Z","shell.execute_reply":"2025-05-02T14:43:58.095090Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# --- Main Execution ---\n\nif __name__ == \"__main__\":\n\n    # --- Load Data ---\n    all_paths = get_file_paths(DATA_DIR)\n    # Check if enough files exist overall\n    # Need SEQ_LEN + 1 for training OR PREDICTION_FILE_INDEX for prediction\n    # The path list is sorted, so the highest index needed is the prediction index file.\n    # If PREDICTION_FILE_INDEX is less than SEQ_LEN+1, we still need SEQ_LEN+1 files for training.\n    # The prediction needs PREDICTION_FILE_INDEX and the SEQ_LEN files preceding it, meaning files from\n    # index (PREDICTION_FILE_INDEX - SEQ_LEN) up to PREDICTION_FILE_INDEX (inclusive of target).\n    # So the highest index file needed is PREDICTION_FILE_INDEX.\n    # The training needs files from index 1 up to SEQ_LEN+1.\n    # We need files up to max(SEQ_LEN + 1, PREDICTION_FILE_INDEX).\n    required_max_index = max(SEQ_LEN + 1, PREDICTION_FILE_INDEX)\n    # Check if the path list contains the file with index required_max_index\n    required_file_path = os.path.join(DATA_DIR, f\"IPF_Final_{required_max_index}.csv\")\n\n    if required_file_path not in all_paths:\n         last_file_index = int(os.path.basename(all_paths[-1]).split('_')[2].split('.')[0]) if all_paths else 0\n         raise FileNotFoundError(f\"Dataset directory '{DATA_DIR}' does not contain enough files. Needed file 'IPF_Final_{required_max_index}.csv' for training or prediction setup. Found files up to index {last_file_index}.\")\n\n\n    x_train_tensor, y_train_tensor = load_training_data(all_paths, SEQ_LEN)\n    x_pred_tensor, y_pred_tensor = load_prediction_data(all_paths, SEQ_LEN, PREDICTION_FILE_INDEX)\n\n    # --- Initialize Model and Guide ---\n    model = BayesianLSTMModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n    # AutoNormal guide approximates the posterior distribution of parameters with Gaussians\n    guide = pyro.infer.autoguide.AutoNormal(model)\n\n    # --- Training Setup ---\n    pyro.clear_param_store() # Clear parameter store before training\n    # ClippedAdam optimizer helps prevent exploding gradients\n    optimizer = ClippedAdam({\"lr\": LEARNING_RATE})\n    # Stochastic Variational Inference (SVI) with Trace Evidence Lower Bound (ELBO) loss\n    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n\n    # --- Training Loop ---\n    print(\"\\n--- Starting Training ---\")\n    best_loss = float('inf')\n    no_improve_count = 0\n\n    for step in range(NUM_STEPS):\n        # Perform one optimization step\n        loss = svi.step(x_train_tensor, y_train_tensor)\n\n        if step % 50 == 0 or step == NUM_STEPS - 1:\n            print(f\"Step {step:>4} - ELBO Loss: {loss:.4f}\")\n\n        # Early stopping check\n        # Use a small tolerance for float comparison\n        if loss < best_loss - MIN_DELTA:\n            best_loss = loss\n            no_improve_count = 0\n            # Optionally: save model parameters here if loss improves\n            # pyro.get_param_store().save(\"best_model.params\")\n        else:\n            no_improve_count += 1\n\n        if no_improve_count >= PATIENCE:\n            print(f\"\\n⏹️ Early stopping triggered at step {step} due to no improvement for {PATIENCE} steps.\")\n            break\n\n    print(\"--- Training Finished ---\")\n\n    # --- Prediction ---\n    print(f\"\\n--- Generating Predictions for file index {PREDICTION_FILE_INDEX} ---\")\n    # Use Predictive to sample from the posterior predictive distribution\n    # It runs the model forward multiple times, sampling from the guide each time\n    predictive = Predictive(model, guide=guide, num_samples=NUM_PREDICTION_SAMPLES)\n\n    # Get predictions for the prediction input tensor\n    # No need for y_pred_tensor here, as we are generating predictions\n    # The model's forward pass handles the device placement of x_pred_tensor\n    posterior_samples = predictive(x_pred_tensor)\n\n    # 'obs' contains samples from the likelihood N(backbone(x), obs_scale)\n    # Shape: (num_samples, batch_size, output_dim)\n    y_pred_samples = posterior_samples['obs'].detach().cpu().numpy()\n\n    # Calculate the mean prediction across samples\n    # Shape: (batch_size, output_dim)\n    y_pred_mean = y_pred_samples.mean(axis=0)\n\n    # Calculate the standard deviation across samples (uncertainty estimate)\n    y_pred_std = y_pred_samples.std(axis=0)\n\n    print(f\"Generated {NUM_PREDICTION_SAMPLES} posterior samples.\")\n    print(f\"Mean prediction shape: {y_pred_mean.shape}\")\n    print(f\"Prediction uncertainty (std dev) shape: {y_pred_std.shape}\")\n\n\n    # --- Evaluation ---\n    print(\"\\n--- Evaluating Predictions ---\")\n    # Get the actual target values\n    y_actual = y_pred_tensor.cpu().numpy() # Shape: (batch_size, output_dim)\n\n    # Calculate RMSE for each output dimension\n    rmse_dim = []\n    for i in range(OUTPUT_DIM):\n        mse_dim_i = mean_squared_error(y_actual[:, i], y_pred_mean[:, i])\n        rmse_dim_i = math.sqrt(mse_dim_i)\n        rmse_dim.append(rmse_dim_i)\n        print(f\"RMSE for Output Dimension {i+1}: {rmse_dim_i:.4f}\")\n\n    # Calculate overall RMSE across all dimensions\n    overall_mse = mean_squared_error(y_actual.flatten(), y_pred_mean.flatten())\n    overall_rmse = math.sqrt(overall_mse)\n    print(f\"Overall RMSE: {overall_rmse:.4f}\")\n\n    # Example: Print first 5 predictions vs actuals\n    print(\"\\n--- Sample Predictions vs Actuals (First 5) ---\")\n    for i in range(min(5, y_actual.shape[0])):\n        print(f\"Sample {i+1}:\")\n        print(f\"  Actual: {np.round(y_actual[i], 3)}\")\n        print(f\"  Predicted Mean: {np.round(y_pred_mean[i], 3)}\")\n        print(f\"  Predicted StdDev: {np.round(y_pred_std[i], 3)}\")\n\n    print(\"\\n--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:01:51.033883Z","iopub.execute_input":"2025-05-02T15:01:51.034541Z","iopub.status.idle":"2025-05-02T15:04:27.136614Z","shell.execute_reply.started":"2025-05-02T15:01:51.034514Z","shell.execute_reply":"2025-05-02T15:04:27.135832Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading training data using sequence length: 18\nTraining X shape: torch.Size([10004, 18, 3]), Training Y shape: torch.Size([10004, 3])\nLoading data for predicting file index: 20\nUsing files IPF_Final_1.csv to IPF_Final_19.csv for prediction input.\nPrediction X shape: torch.Size([10004, 18, 3]), Prediction Y shape: torch.Size([10004, 3])\n\n--- Starting Training ---\nStep    0 - ELBO Loss: 88070484.7034\nStep   50 - ELBO Loss: 83271817.7628\nStep  100 - ELBO Loss: 69819734.4022\nStep  150 - ELBO Loss: 52214511.1879\nStep  200 - ELBO Loss: 42453460.9271\nStep  250 - ELBO Loss: 38147586.9874\nStep  300 - ELBO Loss: 30634857.6202\nStep  350 - ELBO Loss: 24832190.7231\nStep  400 - ELBO Loss: 23197650.4706\nStep  450 - ELBO Loss: 21951287.2434\nStep  500 - ELBO Loss: 19166266.9342\nStep  550 - ELBO Loss: 18170914.3102\nStep  600 - ELBO Loss: 15846260.4510\nStep  650 - ELBO Loss: 15112913.6985\nStep  700 - ELBO Loss: 14359687.0287\nStep  750 - ELBO Loss: 13560104.0832\nStep  800 - ELBO Loss: 12863705.3777\nStep  850 - ELBO Loss: 11534215.0846\nStep  900 - ELBO Loss: 11087307.4327\nStep  950 - ELBO Loss: 9912211.3850\nStep 1000 - ELBO Loss: 8412864.6327\nStep 1050 - ELBO Loss: 7966511.3929\nStep 1100 - ELBO Loss: 7268759.0034\nStep 1150 - ELBO Loss: 6686578.2649\nStep 1200 - ELBO Loss: 6227348.8794\nStep 1250 - ELBO Loss: 5823473.9102\nStep 1300 - ELBO Loss: 5143842.9797\nStep 1350 - ELBO Loss: 4792333.7895\nStep 1400 - ELBO Loss: 4478542.2916\nStep 1450 - ELBO Loss: 4092327.0179\nStep 1500 - ELBO Loss: 3735704.0260\nStep 1550 - ELBO Loss: 3444759.5348\nStep 1600 - ELBO Loss: 3109614.9609\nStep 1650 - ELBO Loss: 2945980.6769\nStep 1700 - ELBO Loss: 2706680.1018\nStep 1750 - ELBO Loss: 2545409.4297\nStep 1800 - ELBO Loss: 2328400.2894\nStep 1850 - ELBO Loss: 2186210.0376\nStep 1900 - ELBO Loss: 2080216.5880\nStep 1950 - ELBO Loss: 1876627.5455\nStep 1999 - ELBO Loss: 1760858.3842\n--- Training Finished ---\n\n--- Generating Predictions for file index 20 ---\nGenerated 100 posterior samples.\nMean prediction shape: (10004, 3)\nPrediction uncertainty (std dev) shape: (10004, 3)\n\n--- Evaluating Predictions ---\nRMSE for Output Dimension 1: 19.8863\nRMSE for Output Dimension 2: 6.9754\nRMSE for Output Dimension 3: 25.9738\nOverall RMSE: 19.3111\n\n--- Sample Predictions vs Actuals (First 5) ---\nSample 1:\n  Actual: [ 70.856 124.277  11.441]\n  Predicted Mean: [ 75.478 126.734   9.837]\n  Predicted StdDev: [1.855 2.169 1.877]\nSample 2:\n  Actual: [ 75.766 130.956  45.862]\n  Predicted Mean: [ 77.766 130.982  48.032]\n  Predicted StdDev: [2.196 2.175 2.061]\nSample 3:\n  Actual: [ 84.132 131.803  47.742]\n  Predicted Mean: [ 84.808 132.171  48.208]\n  Predicted StdDev: [1.936 1.996 2.065]\nSample 4:\n  Actual: [ 86.712 133.253  47.635]\n  Predicted Mean: [ 87.964 133.515  48.142]\n  Predicted StdDev: [2.017 2.175 1.962]\nSample 5:\n  Actual: [ 90.466 135.231  48.834]\n  Predicted Mean: [ 91.183 135.696  49.552]\n  Predicted StdDev: [1.909 1.862 1.952]\n\n--- Script Finished ---\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# --- Save Predictions to CSV ---\nprint(f\"\\n--- Saving Predictions to CSV ---\")\n\n# Define column names for the CSV\n# Separate columns for the mean and standard deviation of each output dimension\nmean_cols = [f'pred_mean_dim{i+1}' for i in range(OUTPUT_DIM)]\nstd_cols = [f'pred_std_dim{i+1}' for i in range(OUTPUT_DIM)]\ncolumn_names = mean_cols + std_cols\n\n# Combine the mean predictions and standard deviations side-by-side\n# The shape will be (batch_size, output_dim * 2)\npredicted_data_with_uncertainty = np.hstack((y_pred_mean, y_pred_std))\n\n# Create a pandas DataFrame from the combined data\ndf_predictions = pd.DataFrame(predicted_data_with_uncertainty, columns=column_names)\n\n# Define the output filename based on the prediction file index\noutput_filename = f\"predictions_IPF_Final_{PREDICTION_FILE_INDEX}.csv\"\n\n# Save the DataFrame to a CSV file\n# index=False prevents pandas from writing the DataFrame index as a column\ndf_predictions.to_csv(output_filename, index=False)\n\nprint(f\"Predictions (mean and std dev) saved to {output_filename}\")\nprint(\"\\n--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:13:49.575216Z","iopub.execute_input":"2025-05-02T15:13:49.575725Z","iopub.status.idle":"2025-05-02T15:13:49.645847Z","shell.execute_reply.started":"2025-05-02T15:13:49.575703Z","shell.execute_reply":"2025-05-02T15:13:49.645306Z"}},"outputs":[{"name":"stdout","text":"\n--- Saving Predictions to CSV ---\nPredictions (mean and std dev) saved to predictions_IPF_Final_20.csv\n\n--- Script Finished ---\n","output_type":"stream"}],"execution_count":27}]}